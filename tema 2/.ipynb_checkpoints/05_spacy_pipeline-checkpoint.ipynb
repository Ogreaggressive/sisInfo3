{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m spacy\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m table\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrsly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\__init__.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MESSAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[43mPrinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\printer.py:56\u001b[0m, in \u001b[0;36mPrinter.__init__\u001b[1;34m(self, pretty, no_print, colors, icons, line_max, animation, animation_ascii, hide_animation, ignore_warnings, env_prefix, timestamp)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty \u001b[38;5;241m=\u001b[39m pretty \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_no_pretty\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_print \u001b[38;5;241m=\u001b[39m no_print\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_color \u001b[38;5;241m=\u001b[39m \u001b[43msupports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_log_friendly\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhide_animation \u001b[38;5;241m=\u001b[39m hide_animation \u001b[38;5;129;01mor\u001b[39;00m env_log_friendly\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_warnings \u001b[38;5;241m=\u001b[39m ignore_warnings\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:264\u001b[0m, in \u001b[0;36msupports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANSICON\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_windows_console_supports_ansi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wasabi\\util.py:236\u001b[0m, in \u001b[0;36m_windows_console_supports_ansi\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ctypes\u001b[38;5;241m.\u001b[39mWinError()\n\u001b[1;32m--> 236\u001b[0m console \u001b[38;5;241m=\u001b[39m msvcrt\u001b[38;5;241m.\u001b[39mget_osfhandle(\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Try to enable ANSI output support\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     flags \u001b[38;5;241m=\u001b[39m GetConsoleMode(console)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:310\u001b[0m, in \u001b[0;36mOutStream.fileno\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_stdstream_copy\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m io\u001b[38;5;241m.\u001b[39mUnsupportedOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: fileno"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "spaCy es una librería de código abierto para el procesamiento avanzado del lenguaje natural; incluye __[modelos ya entrenados en varios idiomas](https://spacy.io/usage/models)__, incluido el __[idioma español](https://spacy.io/models/es)__, que facilitan la extracción de características del texto que dependen del contexto.\n",
    "\n",
    "Puede __[instalar spaCy](https://spacy.io/usage)__ con este comando:\n",
    "\n",
    "<code>conda install -c conda-forge spacy</code>\n",
    "\n",
    "\n",
    "Una vez instalado spaCy puede descargar el modelo de lenguaje más apropiado a sus requerimientos de la siguiente manera\n",
    "\n",
    "<code>python -m spacy download es_core_news_md</code>\n",
    "\n",
    "\n",
    "Es recomendable ejecutar el comando anterior desde un consola con privilegios de administrador para evitar este tipo de errores: ```ERROR: Could not install packages due to an OSError: [WinError 5] Acceso denegado```:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El flujo de procesamiento de texto (pipeline)\n",
    "\n",
    "\n",
    "Para facilitar el procesamient de texto, spaCy crea objetos contenedores que representan elementos como oraciones y palabras. Estos objetos, a su vez, tienen atributos que representan características lingüísticas, como la categoría gramatical (part-of-speech).\n",
    "\n",
    "El pipeline de procesamiento es un serie de funciones aplicadas el Doc para aumentar atributos como etiquetas __[POS](https://es.wikipedia.org/wiki/Etiquetado_gramatical)__, etiquetas de dependencia sintáctica o etiquetas __[NER](https://es.wikipedia.org/wiki/Reconocimiento_de_entidades_nombradas)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La ciudad de Sucre es la capital de Bolivia. La ciudad de La Paz está a 3600 metros sobre el nivel del mar. Microsoft fue multado por práctica monopólicas'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"La ciudad de Sucre es la capital de Bolivia. La ciudad de La Paz está a 3600 metros sobre el nivel del mar. Microsoft fue multado por práctica monopólicas\"\n",
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.es import Spanish\n",
    "nlp = Spanish()\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La          0                        \n",
      "ciudad      0                        \n",
      "de          0                        \n",
      "Sucre       0                        \n",
      "es          0                        \n",
      "la          0                        \n",
      "capital     0                        \n",
      "de          0                        \n",
      "Bolivia     0                        \n",
      ".           1                        \n",
      "La          0                        \n",
      "ciudad      0                        \n",
      "de          0                        \n",
      "La          0                        \n",
      "Paz         0                        \n",
      "está        0                        \n",
      "a           0                        \n",
      "3600        0                        \n",
      "metros      0                        \n",
      "sobre       0                        \n",
      "el          0                        \n",
      "nivel       0                        \n",
      "del         0                        \n",
      "mar         0                        \n",
      ".           1                        \n",
      "Microsoft   0                        \n",
      "fue         0                        \n",
      "multado     0                        \n",
      "por         0                        \n",
      "práctica    0                        \n",
      "monopólicas 0                        \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(texto)\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    is_punct = token.is_punct\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print('{:<12}{:<5}{:<10}{:<10}'.format(token_text, is_punct, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ.E.P.D nuestro querido amigo. Nuestro más sentido pésame a la familia.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sent\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\doc.pyx:889\u001b[0m, in \u001b[0;36msents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Q.E.P.D nuestro querido amigo. Nuestro más sentido pésame a la familia.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anotaciones agregadas por el pipeline de procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000015689BCBE20>), ('morphologizer', <spacy.pipeline.morphologizer.Morphologizer object at 0x0000015689BCBCA0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000015689A8FBA0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000015689E72480>), ('lemmatizer', <spacy.lang.es.lemmatizer.SpanishLemmatizer object at 0x0000015689E88080>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000015689A8FC10>)]\n"
     ]
    }
   ],
   "source": [
    "#Cargar el modelo del idioma español\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma, categoría gramatical y dependencias sintáctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La            0    el          DET       det                 \n",
      "ciudad        0    ciudad      NOUN      nsubj               \n",
      "de            0    de          ADP       case                \n",
      "Sucre         0    Sucre       PROPN     nmod      LOC       \n",
      "es            0    ser         AUX       cop                 \n",
      "la            0    el          DET       det                 \n",
      "capital       0    capital     NOUN      ROOT                \n",
      "de            0    de          ADP       case                \n",
      "Bolivia       0    Bolivia     PROPN     nmod      LOC       \n",
      ".             1    .           PUNCT     punct     LOC       \n",
      "La            0    el          DET       det       LOC       \n",
      "ciudad        0    ciudad      NOUN      nsubj     LOC       \n",
      "de            0    de          ADP       case      LOC       \n",
      "La            0    el          DET       det       LOC       \n",
      "Paz           0    Paz         PROPN     nmod      LOC       \n",
      "está          0    estar       AUX       cop                 \n",
      "a             0    a           ADP       case                \n",
      "3600          0    3600        NUM       nummod              \n",
      "metros        0    metro       NOUN      ROOT                \n",
      "sobre         0    sobre       ADP       case                \n",
      "el            0    el          DET       det                 \n",
      "nivel         0    nivel       NOUN      nmod                \n",
      "del           0    del         ADP       case                \n",
      "mar           0    mar         NOUN      nmod                \n",
      ".             1    .           PUNCT     punct               \n",
      "Microsoft     0    Microsoft   PROPN     nsubj     ORG       \n",
      "fue           0    ser         AUX       aux                 \n",
      "multado       0    multar      VERB      ROOT                \n",
      "por           0    por         ADP       case                \n",
      "práctica      0    práctica    NOUN      obl                 \n",
      "monopólicas   0    monopólica  ADJ       obj                 \n"
     ]
    }
   ],
   "source": [
    "#Tokenize the text and apply each pipeline component in order.\n",
    "doc = nlp(texto)\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag\n",
    "    token_text = token.text\n",
    "    is_punct = token.is_punct\n",
    "    token_lemma = token.lemma_\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    ent_type = token.ent_type_\n",
    "    print('{:<14}{:<5}{:<12}{:<10}{:<10}{:<10}'.format(token_text, is_punct, token_lemma, token_pos, token_dep, ent_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('ORG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El componente 'parser' permite segmentar a nivel de oraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.E.P.D nuestro querido amigo.\n",
      "Nuestro más sentido pésame a la familia.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Q.E.P.D nuestro querido amigo. Nuestro más sentido pésame a la familia.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El componente 'parser' permite identificar sintagmas nominales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nuestro querido amigo, Nuestro más sentido, la familia]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematización\n",
    "La __[lematización](https://es.wikipedia.org/wiki/Lematizaci%C3%B3n)__ es un proceso lingüístico que consiste en, dada una forma flexionada (es decir, en plural, en femenino, conjugada, etc), hallar el lema correspondiente. El lema es la forma que por convenio se acepta como representante de todas las formas flexionadas de una misma palabra. Es decir, el lema de una palabra es la palabra que nos encontraríamos como entrada en un diccionario tradicional: singular para sustantivos, masculino singular para adjetivos, infinitivo para verbos. Por ejemplo, decir es el lema de dije, pero también de diré o dijéramos; guapo es el lema de guapas; mesa es el lema de mesas.\n",
    "\n",
    "**La lematización facilita la búsqueda de palabras porque evita tener que tomar en cuenta las variaciones de ciertas palabras al momento de realizar búsquedas**. Por ejemplo, en lugar de que tener que buscar \"viajando\" o \"viajaba\" sólo tenemos que buscar \"viajar\". \n",
    "\n",
    "**Referencias**\n",
    "\n",
    "- __[Spacy - Lemmatization](https://spacy.io/usage/linguistic-features#lemmatization)__\n",
    "- __[NLP-03 Lemmatization and Stemming using spaCy](https://medium.com/mlearning-ai/nlp-03-lemmatization-and-stemming-using-spacy-b2829becceca)__\n",
    "\n",
    "*Otras referencias*\n",
    "- __[How to build a Lemmatizer](https://medium.com/analytics-vidhya/how-to-build-a-lemmatizer-7aeff7a1208c)__\n",
    "- __[How to solve Spanish lemmatization problems with SpaCy?](https://stackoverflow.com/questions/60534999/how-to-solve-spanish-lemmatization-problems-with-spacy)__\n",
    "- __[spacy-spanish-lemmatizer](https://github.com/pablodms/spacy-spanish-lemmatizer)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estoy (lemma: estar)\n",
      "viajando (lemma: viajar)\n",
      "a (lemma: a)\n",
      "Cochabamba (lemma: Cochabamba)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Estoy viajando a Cochabamba')\n",
    "for token in doc:\n",
    "  print(f\"{token.text} (lemma: {token.lemma_})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregar un caso especial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estoy (lemma: estar)\n",
      "viajando (lemma: viajar)\n",
      "a (lemma: a)\n",
      "Cocha (lemma: Cochabamba)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp.get_pipe('attribute_ruler').add([[{\"TEXT\":\"Cocha\"}]],{\"LEMMA\":\"Cochabamba\"})\n",
    "\n",
    "doc = nlp(u'Estoy viajando a Cocha')\n",
    "\n",
    "for token in doc:\n",
    "  print(f\"{token.text} (lemma: {token.lemma_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Encontrar en el siguiente texto todas las oraciones en las se habla de ```pasear``` y ```perro```\n",
    "\n",
    "```\n",
    "Esos gatos duermen mucho. Los gatos pasean cada Sábado por la mañana. A los perros les gusta pasear en el parque. Mi amigo gana unas monedas paseando los perros de sus vecinos.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "\n",
    "- __[Natural Language Processing With spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/)__\n",
    "- __[Token - Spacy](https://spacy.io/api/token)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
